{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div style=\"border: solid blue 2px; padding: 15px; margin: 10px\">\n",
    "  <b>Overall Summary of the Project – Iteration 1</b><br><br>\n",
    "\n",
    "  Hi Bailey, I’m <b>Victor Camargo</b> (<a href=\"https://hub.tripleten.com/u/e9cc9c11\" target=\"_blank\">TripleTen Hub profile</a>). Thanks for submitting your project — I’m happy to say it is complete and approved.<br><br>\n",
    "\n",
    "  <b>Nice work on:</b><br>\n",
    "  ✔️ Preparing the data properly, dropping irrelevant columns, splitting into train/validation/test sets, and identifying categorical features.<br>\n",
    "  ✔️ Training three different models (Linear Regression, Random Forest, and LightGBM) with appropriate preprocessing and hyperparameters.<br>\n",
    "  ✔️ Measuring and comparing RMSE, training time, and prediction speed, then selecting the best-performing model.<br>\n",
    "  ✔️ Providing a clear and well-structured conclusion with practical takeaways for Rusty Bargain.<br><br>\n",
    "\n",
    "  This is a strong, well-executed project that meets all requirements. Congratulations on the milestone — excellent work.<br><br>\n",
    "\n",
    "  <hr>\n",
    "\n",
    "  🔹 <b>Legend:</b><br>\n",
    "  🟢 Green = well done<br>\n",
    "  🟡 Yellow = suggestions<br>\n",
    "  🔴 Red = must fix<br>\n",
    "  🔵 Blue = your comments or questions<br><br>\n",
    "  \n",
    "  <b>Please ensure</b> that all cells run smoothly from top to bottom and display their outputs before submitting — this helps keep your analysis easy to follow.  \n",
    "  <b>Kind reminder:</b> try not to move, change, or delete reviewer comments, as they are there to track progress and provide better support during your revisions.<br><br>\n",
    "\n",
    "  <b>Feel free to reach out if you need help in Questions channel.</b><br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rusty Bargain used car sales service is developing an app to attract new customers. In that app, you can quickly find out the market value of your car. You have access to historical data: technical specifications, trim versions, and prices. You need to build the model to determine the value. \n",
    "\n",
    "Rusty Bargain is interested in:\n",
    "\n",
    "- the quality of the prediction;\n",
    "- the speed of the prediction;\n",
    "- the time required for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (212621, 11)\n",
      "Valid shape: (70874, 11)\n",
      "Test shape: (70874, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('/datasets/car_data.csv')\n",
    "\n",
    "df = df.drop(['DateCrawled','DateCreated','LastSeen','NumberOfPictures'], axis=1)\n",
    "\n",
    "y = df['Price']\n",
    "X = df.drop('Price', axis=1)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "cat_features = ['VehicleType','Gearbox','Model','FuelType','Brand','NotRepaired']\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Valid shape:\", X_valid.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
    "  Great job on the data preparation step. You correctly dropped irrelevant columns, separated features from the target, and created proper train/validation/test splits with random state set for reproducibility. Also, listing the categorical features here is a good move to streamline model building later.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression | RMSE: 3,499.49 | train: 1.01s | predict: 0.17s\n",
      "   RandomForest | RMSE: 1,856.16 | train: 12.16s | predict: 0.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.venv/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/.venv/lib/python3.9/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['Brand', 'FuelType', 'Gearbox', 'Model', 'NotRepaired', 'VehicleType']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/.venv/lib/python3.9/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/.venv/lib/python3.9/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       LightGBM | RMSE: 1,707.89 | train: 11.20s | predict: 1.31s\n",
      "\n",
      "Summary (lower RMSE is better):\n",
      "      Linear: RMSE=3,499.49 | train=1.01s | predict=0.17s\n",
      "RandomForest: RMSE=1,856.16 | train=12.16s | predict=0.69s\n",
      "    LightGBM: RMSE=1,707.89 | train=11.20s | predict=1.31s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "results = {}\n",
    "\n",
    "cat_cols = ['VehicleType','Gearbox','Model','FuelType','Brand','NotRepaired']\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "def fit_eval(name, model, Xtr, ytr, Xva, yva):\n",
    "    t0 = time.time(); model.fit(Xtr, ytr); train_time = time.time() - t0\n",
    "    t1 = time.time(); preds = model.predict(Xva); pred_time = time.time() - t1\n",
    "    rmse = mean_squared_error(yva, preds, squared=False)\n",
    "    print(f\"{name:>15} | RMSE: {rmse:,.2f} | train: {train_time:.2f}s | predict: {pred_time:.2f}s\")\n",
    "    return rmse, train_time, pred_time\n",
    "\n",
    "prep_ohe = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')), # <-- NEW\n",
    "            ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=True))\n",
    "        ]), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "lr_pipe = Pipeline([('prep', prep_ohe), ('model', LinearRegression())])\n",
    "results['Linear'] = fit_eval('LinearRegression', lr_pipe, X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "prep_ord = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')), # <-- NEW\n",
    "            ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")\n",
    "rf_pipe = Pipeline([\n",
    "    ('prep', prep_ord),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=150,\n",
    "        max_depth=14,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "results['RandomForest'] = fit_eval('RandomForest', rf_pipe, X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    Xtr_lgb = X_train.copy(); Xva_lgb = X_valid.copy()\n",
    "    for c in num_cols:\n",
    "        med = Xtr_lgb[c].median()\n",
    "        Xtr_lgb[c] = Xtr_lgb[c].fillna(med); Xva_lgb[c] = Xva_lgb[c].fillna(med)\n",
    "    for c in cat_cols:\n",
    "        Xtr_lgb[c] = Xtr_lgb[c].astype('object').fillna('missing').astype('category')\n",
    "        Xva_lgb[c] = Xva_lgb[c].astype('object').fillna('missing').astype('category')\n",
    "\n",
    "    lgbm = lgb.LGBMRegressor(\n",
    "        objective='regression',\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=600,\n",
    "        num_leaves=31,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    lgbm.fit(\n",
    "        Xtr_lgb, y_train,\n",
    "        categorical_feature=cat_cols,\n",
    "        eval_set=[(Xva_lgb, y_valid)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=40, verbose=False)]\n",
    "    )\n",
    "    train_time = time.time() - t0\n",
    "    t1 = time.time(); preds = lgbm.predict(Xva_lgb); pred_time = time.time() - t1\n",
    "    rmse = mean_squared_error(y_valid, preds, squared=False)\n",
    "    print(f\"{'LightGBM':>15} | RMSE: {rmse:,.2f} | train: {train_time:.2f}s | predict: {pred_time:.2f}s\")\n",
    "    results['LightGBM'] = (rmse, train_time, pred_time)\n",
    "except Exception as e:\n",
    "    print(\"LightGBM not available, skipping. Details:\", e)\n",
    "\n",
    "print(\"\\nSummary (lower RMSE is better):\")\n",
    "for k, (rmse, tr, pr) in results.items():\n",
    "    print(f\"{k:>12}: RMSE={rmse:,.2f} | train={tr:.2f}s | predict={pr:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Linear', 'RandomForest', 'LightGBM'])\n"
     ]
    }
   ],
   "source": [
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained three different models to compare prediction quality, training speed, and prediction speed:\n",
    "\n",
    "1. **Linear Regression** – serves as a baseline sanity check with OHE encoding.\n",
    "2. **Random Forest Regressor** – a tree-based ensemble model using OHE encoding.\n",
    "3. **LightGBM** – a gradient boosting model that handles categorical features natively.\n",
    "\n",
    "For each model, we measured:\n",
    "- **RMSE** (Root Mean Squared Error) on the validation set, which reflects prediction quality.\n",
    "- **Training time**, since Rusty Bargain is interested in how long it takes to prepare the model.\n",
    "- **Prediction time**, to evaluate speed in a real-time application.\n",
    "\n",
    "These results allow us to identify the trade-off between accuracy and efficiency across the three approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
    "  Excellent implementation of the model training stage. You correctly compared three distinct models: Linear Regression (baseline), Random Forest (tree-based ensemble), and LightGBM (gradient boosting). Each was set up with suitable preprocessing for numerical and categorical data, and you included metrics for RMSE, training time, and prediction speed. This gives a well-rounded comparison that addresses all the project requirements.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Train time (s)</th>\n",
       "      <th>Predict time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>1707.886017</td>\n",
       "      <td>11.199506</td>\n",
       "      <td>1.305753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>1856.159471</td>\n",
       "      <td>12.157575</td>\n",
       "      <td>0.689686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>3499.492992</td>\n",
       "      <td>1.009378</td>\n",
       "      <td>0.171097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     RMSE  Train time (s)  Predict time (s)\n",
       "LightGBM      1707.886017       11.199506          1.305753\n",
       "RandomForest  1856.159471       12.157575          0.689686\n",
       "Linear        3499.492992        1.009378          0.171097"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best on validation: LightGBM (RMSE=1,707.89)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if 'results' not in globals():\n",
    "    raise RuntimeError(\"Run the Model training cell first (it creates the `results` dict).\")\n",
    "\n",
    "df_results = pd.DataFrame(results, index=['RMSE','Train time (s)','Predict time (s)']).T\n",
    "display(df_results.sort_values('RMSE'))\n",
    "\n",
    "best_model_name = df_results['RMSE'].idxmin()\n",
    "best_rmse = df_results.loc[best_model_name, 'RMSE']\n",
    "print(f\"\\nBest on validation: {best_model_name} (RMSE={best_rmse:,.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
    "  Great work on the model analysis stage. You collected the RMSE, training time, and prediction time into a summary DataFrame and sorted by RMSE to clearly highlight the best-performing model. Printing the best model name and its RMSE adds clarity and ensures the results are easy to interpret.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we built and compared models to predict used car prices for Rusty Bargain.\n",
    "The key steps were:\n",
    "\n",
    "1. **Data preparation**\n",
    "- Cleaned the dataset and removed irrelevant columns (`DateCrawled`, `DateCreated`, `LastSeen`, `NumberOfPictures`).\n",
    "- Split the data into training, validation, and test sets.\n",
    "- Identified categorical and numerical features, imputing missing values appropriately.\n",
    "\n",
    "2. **Model training**\n",
    "- Trained three baseline models with pipelines:\n",
    "- **Linear Regression** (with OHE) as a simple baseline.\n",
    "- **Random Forest Regressor** (with OrdinalEncoder for categoricals).\n",
    "- **LightGBM** (handling categorical features natively, with early stopping).\n",
    "- Collected metrics on **RMSE**, **training time**, and **prediction time**.\n",
    "\n",
    "3. **Model analysis**\n",
    "- Results on the validation set:\n",
    "- **LightGBM**: RMSE ≈ 1,707.89 (best), training ≈ 9.26s, prediction ≈ 1.20s\n",
    "- **Random Forest**: RMSE ≈ 1,856.16, training ≈ 10.70s, prediction ≈ 0.66s\n",
    "- **Linear Regression**: RMSE ≈ 3,499.49 (worst), training ≈ 0.71s, prediction ≈ 0.17s\n",
    "- LightGBM achieved the lowest RMSE, meaning it delivered the most accurate predictions.\n",
    "- Random Forest performed reasonably well but required more time and gave higher error.\n",
    "- Linear Regression served only as a sanity check baseline and was not competitive.\n",
    "\n",
    "4. **Final evaluation**\n",
    "- Based on the validation results, **LightGBM** was chosen as the best model.\n",
    "- It provides a good balance of accuracy and speed, making it suitable for real-time pricing in Rusty Bargain’s app.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "- **LightGBM is the most effective model**, achieving the lowest RMSE with acceptable training and prediction times.\n",
    "- **Random Forest** is a solid alternative but less accurate.\n",
    "- **Linear Regression** is too simplistic for this task, confirming the need for more advanced methods.\n",
    "- Rusty Bargain can deploy the LightGBM model to provide reliable car price predictions to users, improving customer trust and engagement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
    "  Excellent job on the conclusion and final evaluation. You summarized the entire workflow clearly, compared the models with their RMSE and timing metrics, and selected LightGBM as the best option with solid justification. The takeaways section is well written and highlights the practical impact of your results for Rusty Bargain.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 'x' to check. Then press Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook is open\n",
    "- [x]  Code is error free\n",
    "- [x]  The cells with the code have been arranged in order of execution\n",
    "- [x]  The data has been downloaded and prepared\n",
    "- [x]  The models have been trained\n",
    "- [x]  The analysis of speed and quality of the models has been performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested three models to predict used car prices for Rusty Bargain.\n",
    "LightGBM gave the best balance of accuracy and speed, achieving the lowest error (RMSE ≈ 1,708).\n",
    "This model is recommended for deployment in the app to provide fast and reliable price estimates for customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
